"""
RAG Chat Application with Streamlit
Main application file for document upload and chat interface
"""

import streamlit as st
import os
from dotenv import load_dotenv
from document_processor import process_document
from vector_store import VectorStore
from openai import OpenAI
import config

# Load environment variables
load_dotenv()

# Page configuration
st.set_page_config(
    page_title="RAG Chat Application",
    page_icon="ğŸ“š",
    layout="wide"
)

# Initialize session state
if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'documents_processed' not in st.session_state:
    st.session_state.documents_processed = 0
if 'openai_client' not in st.session_state:
    st.session_state.openai_client = None


def initialize_clients():
    """Initialize OpenAI and Pinecone clients"""
    try:
        openai_api_key = os.getenv('OPENAI_API_KEY')
        pinecone_api_key = os.getenv('PINECONE_API_KEY')
        pinecone_index_name = os.getenv('PINECONE_INDEX_NAME', 'rag-documents')
        
        if not all([openai_api_key, pinecone_api_key]):
            st.error("âŒ API keys bulunamadÄ±! LÃ¼tfen .env dosyasÄ±nÄ± kontrol edin.")
            st.info("ğŸ“ .env dosyasÄ±nda ÅŸu deÄŸiÅŸkenler olmalÄ±: OPENAI_API_KEY, PINECONE_API_KEY")
            return False
        
        # Initialize vector store
        if st.session_state.vector_store is None:
            st.session_state.vector_store = VectorStore(
                api_key=pinecone_api_key,
                index_name=pinecone_index_name,
                openai_api_key=openai_api_key
            )
        
        # Initialize OpenAI client
        if st.session_state.openai_client is None:
            st.session_state.openai_client = OpenAI(api_key=openai_api_key)
        
        return True
    except Exception as e:
        st.error(f"âŒ BaÄŸlantÄ± hatasÄ±: {str(e)}")
        return False


def generate_rag_response(query: str) -> str:
    """
    Generate response using enhanced RAG pipeline with reasoning
    Optimized for o1-preview model's deep analytical capabilities
    
    Args:
        query: User's question
        
    Returns:
        AI-generated response with reasoning
    """
    try:
        # Query vector store for relevant chunks
        relevant_chunks = st.session_state.vector_store.query_vectors(
            query_text=query,
            top_k=config.TOP_K
        )
        
        if not relevant_chunks:
            return "ÃœzgÃ¼nÃ¼m, yÃ¼klediÄŸiniz dokÃ¼manlarda bu soruyla ilgili bilgi bulamadÄ±m. LÃ¼tfen Ã¶nce dokÃ¼man yÃ¼klediÄŸinizden emin olun."
        
        # Filter by similarity threshold
        filtered_chunks = [
            chunk for chunk in relevant_chunks 
            if chunk['score'] >= config.SIMILARITY_THRESHOLD
        ]
        
        if not filtered_chunks:
            # Threshold Ã§ok yÃ¼ksekse, en iyi sonuÃ§larÄ± kullan
            if relevant_chunks:
                st.warning(f"âš ï¸ DÃ¼ÅŸÃ¼k benzerlik (en yÃ¼ksek: {relevant_chunks[0]['score']:.2%}). En iyi {len(relevant_chunks[:3])} sonuÃ§ kullanÄ±lÄ±yor.")
                filtered_chunks = relevant_chunks[:3]  # En az 3 chunk kullan
            else:
                return "DokÃ¼manlarda bu soruyla ilgili bilgi bulunamadÄ±. LÃ¼tfen sorunuzu farklÄ± ÅŸekilde sormayÄ± deneyin."
        
        # Build enriched context with structure
        context_parts = []
        sources = []
        
        for i, chunk in enumerate(filtered_chunks):
            context_parts.append(
                f"=== KAYNAK {i+1} ===\n"
                f"Dosya: {chunk['filename']}\n"
                f"Ä°lgililik Skoru: {chunk['score']:.3f}\n"
                f"Ä°Ã§erik:\n{chunk['text']}\n"
            )
            sources.append({
                'filename': chunk['filename'],
                'score': chunk['score']
            })
        
        context = "\n\n".join(context_parts)
        
        # Limit context but preserve important info
        if len(context) > config.MAX_CONTEXT_LENGTH:
            context = context[:config.MAX_CONTEXT_LENGTH] + "\n\n[... iÃ§erik uzunluk nedeniyle kÄ±saltÄ±ldÄ± ...]"
        
        # Enhanced prompt for GPT-5 reasoning
        system_prompt = """Sen bir uzman RAG (Retrieval Augmented Generation) asistanÄ±sÄ±n ve Ä°nsan KaynaklarÄ±/CV analizi konusunda uzmansÄ±n. 

GÃ–REV:
1. Verilen dokÃ¼man iÃ§eriÄŸini DÄ°KKATLÄ°CE ve DETAYLI analiz et
2. KullanÄ±cÄ±nÄ±n sorusuyla Ä°LGÄ°LÄ° TÃœM bilgileri belirle ve BÄ°RLEÅTÄ°R
3. Birden fazla dokÃ¼mandan gelen bilgileri SENTEZLE
4. KiÅŸi isimleri, pozisyonlar, beceriler, deneyimler gibi detaylarÄ± DÄ°KKATLÄ°CE NOT ET
5. SayÄ±sal sorularda (kaÃ§ kiÅŸi, kaÃ§ yÄ±l, vb.) DÄ°KKATLÄ° SAY ve doÄŸru rakam ver
6. Liste sorularÄ± iÃ§in KAPSAMLI listeler oluÅŸtur
7. Verilen kaynaklara dayalÄ± mantÄ±ksal Ã‡IKARIMLAR yap
8. YanÄ±tlarÄ±nÄ± TÃ¼rkÃ§e, net, yapÄ±landÄ±rÄ±lmÄ±ÅŸ ve profesyonel ÅŸekilde yaz

CEVAP FORMATI:
- Net ve direkt cevap ver
- Ã–rnekler ve isimler kullan
- Listelerde madde iÅŸareti kullan
- SayÄ±larÄ± ve istatistikleri vurgula

Senin gÃ¼cÃ¼n: CV analizi, kiÅŸi eÅŸleÅŸtirme, beceri deÄŸerlendirme, derin analiz ve kapsamlÄ± sentez."""

        user_prompt = f"""AÅŸaÄŸÄ±da {len(filtered_chunks)} farklÄ± dokÃ¼man iÃ§eriÄŸi var. BunlarÄ± analiz ederek soruyu yanÄ±tla.

=== DOKÃœMAN Ä°Ã‡ERÄ°KLERÄ° ===
{context}

=== KULLANICI SORUSU ===
{query}

=== TALÄ°MATLAR ===
- TÃœM dokÃ¼manlardaki ilgili bilgileri BÄ°RLEÅTÄ°R
- SayÄ±sal sorularda (kaÃ§ kiÅŸi, kaÃ§ tane vb.) DÄ°KKAT ET ve doÄŸru say
- Ä°simleri, pozisyonlarÄ±, becerileri belirgin ÅŸekilde BELIRT
- Liste sorularÄ± iÃ§in KAPSAMLI listeler oluÅŸtur
- Her bilgi iÃ§in KAYNAK dosyayÄ± referans ver
- Net, yapÄ±landÄ±rÄ±lmÄ±ÅŸ ve DETAYLI cevap ver

ÅÄ°MDÄ° YANIT VER:"""
        
        # Generate response using OpenAI GPT-5
        # GPT-5 has specific API requirements
        try:
            # GPT-5 does not support custom temperature, only default (1)
            if config.CHAT_MODEL.startswith("gpt-5"):
                response = st.session_state.openai_client.chat.completions.create(
                    model=config.CHAT_MODEL,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_completion_tokens=config.MAX_COMPLETION_TOKENS
                    # temperature not supported for GPT-5
                )
            else:
                # Other models support temperature
                response = st.session_state.openai_client.chat.completions.create(
                    model=config.CHAT_MODEL,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=config.TEMPERATURE,
                    max_completion_tokens=config.MAX_COMPLETION_TOKENS
                )
        except Exception as api_error:
            error_str = str(api_error)
            # Fallback for max_tokens parameter
            if "max_completion_tokens" in error_str:
                response = st.session_state.openai_client.chat.completions.create(
                    model=config.CHAT_MODEL,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=config.TEMPERATURE if not config.CHAT_MODEL.startswith("gpt-5") else 1,
                    max_tokens=config.MAX_COMPLETION_TOKENS
                )
            else:
                raise
        
        answer = response.choices[0].message.content
        
        # Debug: Check if answer is empty
        if not answer or answer.strip() == "":
            answer = "âš ï¸ Model cevap Ã¼retemedi. LÃ¼tfen sorunuzu daha detaylÄ± sorun veya farklÄ± ÅŸekilde ifade edin."
        
        # Add enhanced sources section with unique filenames
        # Group by filename and keep the highest score
        unique_sources = {}
        for source in sources:
            filename = source['filename']
            score = source['score']
            if filename not in unique_sources or score > unique_sources[filename]:
                unique_sources[filename] = score
        
        # Sort by score (descending)
        sorted_sources = sorted(unique_sources.items(), key=lambda x: x[1], reverse=True)
        
        sources_text = "\n\n---\n**ğŸ“š KullanÄ±lan Kaynaklar:**\n"
        for i, (filename, score) in enumerate(sorted_sources[:5], 1):
            sources_text += f"{i}. {filename} (Ä°lgililik: {score:.1%})\n"
        
        # Make sure answer is shown before sources
        full_response = f"{answer}\n{sources_text}"
        
        return full_response
        
    except Exception as e:
        error_msg = str(e)
        
        # Log the full error for debugging
        import traceback
        full_error = traceback.format_exc()
        
        if "model_not_found" in error_msg or "does not exist" in error_msg:
            return f"""âš ï¸ **GPT-5 Model BulunamadÄ±!**

API key'iniz GPT-5'e eriÅŸimi desteklemiyor olabilir.

**Ã‡Ã–ZÃœM:** `config.py` dosyasÄ±nda modeli deÄŸiÅŸtirin:
```python
CHAT_MODEL = "gpt-4o"  # Bu kesin Ã§alÄ±ÅŸÄ±r
```

Sonra Streamlit'i yeniden baÅŸlatÄ±n (Ctrl+C, sonra `streamlit run app.py`)

Hata detayÄ±: {error_msg}"""
        
        return f"""âŒ **Cevap Ãœretme HatasÄ±**

{error_msg}

**Debug Bilgisi:**
- Model: {config.CHAT_MODEL}
- Context uzunluÄŸu: {len(context) if 'context' in locals() else 'N/A'} karakter
- Chunk sayÄ±sÄ±: {len(filtered_chunks) if 'filtered_chunks' in locals() else 'N/A'}

LÃ¼tfen terminal Ã§Ä±ktÄ±sÄ±nÄ± kontrol edin veya farklÄ± bir model deneyin."""


# Main UI
st.title("ğŸ“š RAG Chat UygulamasÄ±")
st.markdown("DokÃ¼manlarÄ±nÄ±zÄ± yÃ¼kleyin ve sorularÄ±nÄ±zÄ± sorun!")

# Initialize clients
if not initialize_clients():
    st.stop()

# Sidebar for document upload
with st.sidebar:
    st.header("ğŸ“„ DokÃ¼man YÃ¶netimi")
    
    # File uploader
    uploaded_files = st.file_uploader(
        "DokÃ¼manlarÄ± yÃ¼kleyin (PDF, TXT, DOCX)",
        type=['pdf', 'txt', 'docx'],
        accept_multiple_files=True
    )
    
    # Process button
    if uploaded_files:
        st.info(f"ğŸ“‹ {len(uploaded_files)} dosya seÃ§ildi")
        
        if st.button("ğŸš€ DokÃ¼manlarÄ± Ä°ÅŸle ve Kaydet", type="primary"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            total_chunks = 0
            
            for idx, file in enumerate(uploaded_files):
                try:
                    status_text.text(f"Ä°ÅŸleniyor: {file.name}")
                    
                    # Reset file pointer
                    file.seek(0)
                    
                    # Process document
                    chunks = process_document(file, file.name)
                    
                    if not chunks:
                        st.warning(f"âš ï¸ {file.name}: BoÅŸ dokÃ¼man, atlanÄ±yor")
                        continue
                    
                    # Embed and store
                    num_chunks = st.session_state.vector_store.embed_and_store(chunks)
                    total_chunks += num_chunks
                    
                    st.success(f"âœ… {file.name}: {num_chunks} chunk iÅŸlendi")
                    
                    progress_bar.progress((idx + 1) / len(uploaded_files))
                    
                except Exception as e:
                    error_msg = str(e)
                    if "dimension" in error_msg.lower() and "1536" in error_msg:
                        st.error(f"""âŒ **DIMENSION HATASI!**
                        
Pinecone index'iniz eski dimension'da (1536). Yeni embedding modeli 3072 kullanÄ±yor.

**Ã‡Ã–ZÃœM:** .env dosyanÄ±zda index adÄ±nÄ± deÄŸiÅŸtirin:
```
PINECONE_INDEX_NAME=rag-documents-v2
```
Sonra sayfayÄ± yenileyin (F5) ve tekrar deneyin.""")
                        break
                    else:
                        st.error(f"âŒ {file.name} iÅŸlenirken hata: {str(e)}")
            
            status_text.empty()
            progress_bar.empty()
            
            st.session_state.documents_processed += len(uploaded_files)
            st.success(f"âœ… {len(uploaded_files)} dokÃ¼man baÅŸarÄ±yla iÅŸlendi! ({total_chunks} chunk)")
    
    st.divider()
    
    # Statistics
    st.subheader("ğŸ“Š Ä°statistikler")
    try:
        stats = st.session_state.vector_store.get_index_stats()
        st.metric("Toplam Vector", stats['total_vectors'])
        st.metric("Ä°ÅŸlenen DokÃ¼man", st.session_state.documents_processed)
    except:
        st.info("Ä°statistikler yÃ¼kleniyor...")
    
    st.divider()
    
    # Clear data button
    if st.button("ğŸ—‘ï¸ TÃ¼m Verileri Temizle", type="secondary"):
        if st.session_state.vector_store:
            st.session_state.vector_store.delete_all_vectors()
            st.session_state.chat_history = []
            st.session_state.documents_processed = 0
            st.success("âœ… TÃ¼m veriler temizlendi!")
            st.rerun()

# Main chat area
st.header("ğŸ’¬ Soru Sorun")

# Display chat history
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Chat input
if prompt := st.chat_input("DokÃ¼manlardaki bilgiler hakkÄ±nda bir soru sorun..."):
    # Add user message to chat history
    st.session_state.chat_history.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Generate and display assistant response
    with st.chat_message("assistant"):
        with st.spinner("DÃ¼ÅŸÃ¼nÃ¼yorum..."):
            response = generate_rag_response(prompt)
        st.markdown(response)
    
    # Add assistant response to chat history
    st.session_state.chat_history.append({"role": "assistant", "content": response})

# Instructions if no documents
if st.session_state.documents_processed == 0:
    st.info("""
    ğŸ‘‹ **HoÅŸ geldiniz!**
    
    BaÅŸlamak iÃ§in:
    1. Sol taraftaki menÃ¼den dokÃ¼manlarÄ±nÄ±zÄ± yÃ¼kleyin (PDF, TXT, DOCX)
    2. "DokÃ¼manlarÄ± Ä°ÅŸle ve Kaydet" butonuna tÄ±klayÄ±n
    3. DokÃ¼manlarÄ±nÄ±z hakkÄ±nda sorular sormaya baÅŸlayÄ±n!
    """)

